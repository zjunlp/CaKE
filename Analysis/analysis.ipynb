{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from patch_activations import create_target_prompt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from classify_results import load_activation_patching, classify_generations, find_layers_by_classification\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from utils import get_answers, check_answer_in_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/Qwen/Qwen2.5-7B-Instruct/two_hop.csv')\n",
    "no_shortcut_correct = df[df['composition_correct'] & df['first_hop_correct'] & df['second_hop_correct'] & ~df['entity_shortcut_correct'] & ~df['relation_shortcut_correct']]\n",
    "no_shortcut_incorrect = df[~df['composition_correct'] & df['first_hop_correct'] & df['second_hop_correct'] & ~df['entity_shortcut_correct'] & ~df['relation_shortcut_correct']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_second_hop = []\n",
    "for index, row in no_shortcut_correct.iterrows():\n",
    "    r1_template = row['r1_template']\n",
    "    r2_template = row['r2_template']\n",
    "    source_prompt = row['source_prompt']\n",
    "    e1 = row['e1_label']\n",
    "    e2 = row['e2_label']\n",
    "    e2_type = row['e2_type']\n",
    "    r2_type = row['r2_type']\n",
    "    r1_type = row['r1_type']\n",
    "    e3_label = row['e3_label']\n",
    "    correct_second_hop.append((e2, r2_type, e3_label))\n",
    "correct_second_hop = list(set(correct_second_hop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_shortcut_incorrect[\"is_correct_second_hop\"] = no_shortcut_incorrect.apply(\n",
    "    lambda row: (row[\"e2_label\"], row[\"r2_type\"], row[\"e3_label\"]) in correct_second_hop,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 筛选出符合条件的行\n",
    "filtered_df = no_shortcut_incorrect[no_shortcut_incorrect[\"is_correct_second_hop\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_group(group, n=100):\n",
    "    # 如果组的样本数小于 n，则取全部样本\n",
    "    return group.sample(n=min(len(group), n), random_state=42)  # random_state 保证可重复性\n",
    "\n",
    "# 按 (r1_type, r2_type) 分组，并对每组进行采样\n",
    "sampled_df = no_shortcut_correct.groupby([\"r1_type\", \"r2_type\"], group_keys=False).apply(sample_group)\n",
    "sampled_df.to_csv('./datasets/Qwen/Qwen2.5-7B-Instruct/correct.csv', index=False)\n",
    "print(len(sampled_df))\n",
    "sampled_df = no_shortcut_incorrect[~no_shortcut_incorrect[\"is_correct_second_hop\"]].groupby([\"r1_type\", \"r2_type\"], group_keys=False).apply(sample_group,n=5)\n",
    "sampled_df.to_csv('./datasets/Qwen/Qwen2.5-7B-Instruct/incorrect.csv', index=False)\n",
    "print(len(sampled_df))\n",
    "sampled_df = no_shortcut_incorrect[no_shortcut_incorrect[\"is_correct_second_hop\"]].groupby([\"r1_type\", \"r2_type\"], group_keys=False).apply(sample_group,n=30)\n",
    "sampled_df.to_csv('./datasets/Qwen/Qwen2.5-7B-Instruct/inconsistent.csv', index=False)\n",
    "print(len(sampled_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for Entity Patch and Relation Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "model_name = 'Qwen/Qwen2.5-7B-Instruct'\n",
    "df = pd.read_csv(f'{model_name}/no_shortcut_correct.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_centric = {}\n",
    "relation_centric = {}\n",
    "relation_template = {}\n",
    "aliases = {}\n",
    "for index, row in df.iterrows():\n",
    "    e2_label = row['e2_label']\n",
    "    r2_type = row['r2_type']\n",
    "    if e2_label not in entity_centric:\n",
    "        entity_centric[e2_label] = set()\n",
    "    entity_centric[e2_label].add((r2_type,row['e3_label']))\n",
    "    if r2_type not in relation_centric:\n",
    "        relation_centric[r2_type] = set()\n",
    "        relation_template[r2_type] = row['r2_template']\n",
    "    relation_centric[r2_type].add((e2_label,row['e3_label']))\n",
    "    if e2_label not in aliases:\n",
    "        aliases[e2_label] = row['e2_aliases']\n",
    "    if row['e3_label'] not in aliases:\n",
    "        aliases[row['e3_label']] = row['e3_aliases']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "relation_patch_rows = []\n",
    "for index, row in df.iterrows():\n",
    "    e2_label = row['e2_label']\n",
    "    if len(entity_centric[e2_label]) == 1:\n",
    "        continue\n",
    "    valid_items = [item for item in entity_centric[e2_label] \n",
    "                  if item[0] != row['r2_type']]\n",
    "    if not valid_items:\n",
    "        continue\n",
    "    random_item = random.choice(valid_items)\n",
    "    patch_r = random_item[0]\n",
    "    patch_label = random_item[1]\n",
    "    patch_aliases = aliases[patch_label]\n",
    "    row['patch_r'] = patch_r\n",
    "    row['patch_label'] = patch_label\n",
    "    row['patch_aliases'] = patch_aliases\n",
    "    if row['patch_r'] in relation_template[patch_r]:\n",
    "        row['patch_r_template'] = relation_template[patch_r]\n",
    "    else:\n",
    "        continue\n",
    "    relation_patch_rows.append(row)\n",
    "relation_patch_df = pd.DataFrame(relation_patch_rows)\n",
    "relation_patch_df.to_csv(f'{model_name}/relation_patch.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_patch_rows = []\n",
    "for index, row in df.iterrows():\n",
    "    r2_type = row['r2_type']\n",
    "    for item in relation_centric[r2_type]:\n",
    "        if item[0] == row['e2_label']:\n",
    "            continue\n",
    "        random_item = item\n",
    "        break\n",
    "    patch_e = random_item[0]\n",
    "    patch_label = random_item[1]\n",
    "    patch_aliases = aliases[patch_label]\n",
    "    row['patch_e'] = patch_e\n",
    "    row['patch_label'] = patch_label\n",
    "    row['patch_aliases'] = patch_aliases\n",
    "\n",
    "    entity_patch_rows.append(row)\n",
    "entity_patch_df = pd.DataFrame(entity_patch_rows)\n",
    "entity_patch_df.to_csv(f'{model_name}/entity_patch.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity and Relation Patch Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_prediction_correct_row(row):\n",
    "    if pd.isna(row[\"generation\"]):\n",
    "        return False\n",
    "    generation = row[\"generation\"]\n",
    "    e3_answers = get_answers(row, \"patch\")[\"patch_answers\"]\n",
    "    if check_answer_in_pred(generation, e3_answers):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def classify_prediction_correct(generations, target=None):\n",
    "    return generations.apply(classify_prediction_correct_row, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entity_relation_patching(dataset_dir):\n",
    "    generations = defaultdict(dict)\n",
    "    layers = defaultdict(dict)\n",
    "    for source in [\"patch_e\", \"patch_r\"]:\n",
    "        path = f\"{dataset_dir}/entity_relation_patching/{source}_test_patching.csv\"\n",
    "        if Path(path).exists():\n",
    "            generations[source] = classify_generations(path, classify_prediction_correct, prev_layers=False)\n",
    "            layers[source] = find_layers_by_classification(generations[source],\n",
    "                                                                    True,\n",
    "                                                                    f\"{source}_test_patching_layer\",\n",
    "                                                                    False)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = f\"../datasets/{model_name}\"\n",
    "layers = load_entity_relation_patching(dataset_dir)\n",
    "layers_1 = layers[\"patch_r\"].merge(layers[\"patch_e\"], on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [layers_1]:\n",
    "    for col in [ \"patch_r_test_patching_layer\",\"patch_e_test_patching_layer\"]: #,\n",
    "        df[col] = df[col].apply(lambda x: min(x) if type(x) is list and len(x) > 0 else np.nan)\n",
    "def get_stage_layers(df):\n",
    "    return pd.DataFrame({\n",
    "        \"stage\": [ \"patch_r_test_patching_layer\"],\n",
    "        \"proportion\": [df[\"patch_r_test_patching_layer\"].count() / len(df)],\n",
    "        \"layer\": [df[\"patch_r_test_patching_layer\"].mean()],\n",
    "    })\n",
    "get_stage_layers(layers_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "case_name = \"inconsistent\"\n",
    "dataset_dir = f\"../datasets/{model_name}\"\n",
    "inconsistent_layers = load_activation_patching(dataset_dir, case_name)\n",
    "case_name = \"incorrect\"\n",
    "dataset_dir = f\"../datasets/{model_name}\"\n",
    "incorrect_layers = load_activation_patching(dataset_dir, case_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_1 =incorrect_layers[\"e1\"][\"last\"]\n",
    "layers_2 =inconsistent_layers[\"e1\"][\"last\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [layers_1, layers_2]:\n",
    "        for col in [\"e1_last_activation_patching_layer\"]:\n",
    "            df[col] = df[col].apply(lambda x: min(x) if type(x) is list and len(x) > 0 else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stage_layers(df):\n",
    "    return pd.DataFrame({\n",
    "        \"stage\": [\"e1_last_activation_patching_layer\"],\n",
    "        \"proportion\": [df[\"e1_last_activation_patching_layer\"].count() / len(df),\n",
    "                       ],\n",
    "        \"layer\": [df[\"e1_last_activation_patching_layer\"].mean()],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stage_layers(layers_1)\n",
    "get_stage_layers(layers_2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
